# -*- coding: utf-8 -*-
"""2-й слой нейронной сети. Функция потерь для двумерной области. Методология научных исследований.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MP7GKdo5edMddXUvNauC2Q8MRLhnqpfv
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt

#from my_optimizer import optimizer_step

from matplotlib import pyplot as plt
from matplotlib import pyplot, cm
from mpl_toolkits.mplot3d import Axes3D

# print("CUDA GPU:", torch.cuda.is_available())
# if torch.cuda.is_available():
#    x = torch.ones(20)
#    print(x.device)
#    x = x.to("cuda:0")
#    # or x=x.to("cuda")
# print(x)
# print(x.device)

def f(x):
    return 0.

def analytic_solution(x):
    sol =  (1 / (np.exp(np.pi) - np.exp(-np.pi))) * \
           np.sin(np.pi * x[0]) * (np.exp(np.pi * x[1]) - np.exp(-np.pi * x[1]))
    return sol


from loss import loss_function

# from google.colab import drive
# drive.mount('/content/drive')
#
# # Commented out IPython magic to ensure Python compatibility.
# import os
# from glob import glob
# os.chdir("/content/drive/MyDrive/random_weights")
# # %pwd

import numpy as np

w0 = np.loadtxt('w0.out')
w1 = np.loadtxt('w1.out')

import torch.nn as nn
# нейронная сеть для решения уравнения Пуассона
class PDEnet(nn.Module):
    def __init__(self,N):
        super(PDEnet,self).__init__()
        self.N = N
        fc1 = nn.Linear(2,self.N) # первый слой
        #fc1.weight = nn.Parameter(torch.from_numpy(w0).T.double())
        # fc1.bias = nn.Parameter(torch.zeros(fc1.bias.shape).double())
        fc2 = nn.Linear(self.N, 1) # второй слой
        #fc2.weight = nn.Parameter(torch.from_numpy(w1).reshape(1,w1.shape[0]).double())
        # fc2.bias = nn.Parameter(torch.zeros(fc2.bias.shape).double())
        self.fc1 = fc1

        self.fc2 = fc2

    def forward(self,x):
        x = x.reshape(1, 2)
        y = self.fc1(x)
        y = torch.sigmoid(y)
        y = self.fc2(y.reshape(1, self.N))
        return y

def A(x):
#    return (x[1] * torch.sin(np.pi * x[0]))
     if x[1] == 1.0:
         return torch.sin(np.pi * x[0])
     else:
         if x[1] == 0.0:
             return 0.0
         else:
             return x[1] * torch.sin(np.pi * x[0])


def psy_trial(x, net_out):
    return A(x) + x[0] * (1 - x[0]) * x[1] * (1 - x[1]) * net_out

nx = 10
ny = nx
pde = PDEnet(nx)
dx = 1. / nx
dy = 1. / ny

x_space = torch.linspace(0, 1, nx).double()
y_space = torch.linspace(0, 1, ny).double()
print("CUDA GPU:", torch.cuda.is_available())
if torch.cuda.is_available():
  print(torch.device)
  print(x_space.device)
  x_space.to("cuda:0")
  #y_space.to("cuda:0")
  #print(x_space)
  print(x_space.device)

input_point = torch.zeros(2)#.double()

input_point.shape,pde.fc1.weight.T.shape

pde.fc1.weight.dtype,input_point.dtype,pde.fc1.bias.dtype

torch.matmul(input_point,pde.fc1.weight.T)

out1 = pde.fc1(input_point)
out1

#torch.matmul(out1.reshape(1,3),pde.fc2.weight.reshape(1,3).T)

pde.fc2(out1)

net_out = pde.forward(input_point)
net_out

psy_t = psy_trial(input_point, net_out)
print(psy_t)

loss = loss_function(x_space, y_space,pde,psy_trial,f)
loss.backward()
print(x_space.grad)

lmb = 0.001
optimizer = torch.optim.SGD(pde.parameters(), lr=lmb)
import time
t1 = time.time()
for i in range(100):
    #print('begin ',i,loss.item())
    optimizer.zero_grad()
    #print('zero grad ',i,loss.item())
    #print(x_space.device,y_space.device)
    loss = loss_function(x_space, y_space,pde,psy_trial,f)
    #print(loss.device)
    print(i,loss.item())
    loss.backward(retain_graph=True)
    #print('loop end ',i,loss.item())
    optimizer.step()

    print('step ',i,loss.item())
t2 = time.time()
print('computation time ',t2-t1)

surface = np.zeros((nx,ny))
an_surface = np.zeros((nx,ny))
for i, x in enumerate(x_space):
    for j, y in enumerate(y_space):
        input_point = torch.Tensor([x, y])
        input_point.requires_grad_()
        net_out = pde.forward(input_point)

        psy_t = psy_trial(input_point, net_out)
        surface[i][j] = psy_t
        an_surface[i][j] = analytic_solution([x, y])
diff = np.max(np.abs(surface-an_surface))
print(diff)

import matplotlib.pyplot as plt
import numpy as np
fig = plt.figure()
fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
X, Y = np.meshgrid(x_space.numpy(), y_space.numpy())
surf = ax.plot_surface(X, Y, surface, rstride=1, cstride=1, cmap=cm.viridis,
                       linewidth=0, antialiased=False)
plt.title('Neural solution')
plt.savefig('neural.png')

fig = plt.figure()
fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
X, Y = np.meshgrid(x_space.numpy(), y_space.numpy())
surf = ax.plot_surface(X, Y, an_surface, rstride=1, cstride=1, cmap=cm.viridis,
                       linewidth=0, antialiased=False)
plt.title('Analytic solution')
plt.savefig('analytic.png')

from sklearn.metrics import mean_absolute_percentage_error
mape = mean_absolute_percentage_error(an_surface, surface)
from sklearn.metrics import mean_absolute_error
mae  = mean_absolute_error(an_surface, surface)
print(mape,mae)

import numpy as np
diff = np.abs(an_surface-surface)
md = np.max(diff)
md

np.where(diff == md)

print(surface[5][7],an_surface[5][7],np.abs(surface[5][7]-an_surface[5][7]),np.abs(surface[5][7]-an_surface[5][7])/np.abs(an_surface[5][7]))